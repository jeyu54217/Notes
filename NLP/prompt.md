

Give Direction: Describe the desired style in detail, or reference a relevant persona.

Specify Format: Define what rules to follow, and the required structure of the response.

Provide Examples: Insert a diverse set of test cases where the task was done correctly.

Evaluate Quality: Identify errors and rate responses, testing what drives performance.

Divide Labor: Split tasks into multiple steps, chained together for complex goals.

LLMs work by continuously predicting the next token (~3/4 of a word), starting from what was in your prompt. Each new token is selected based on its probability of appearing next, with an element of randomness (controlled by the temperature parameter), as demonstrated in Figure 1-1.
