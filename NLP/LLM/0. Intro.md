

# LLM
## Definition
## Strengths and Limitations
## Use Cases
##

# Ingredients of an LLM
## Pre-training Data

### 1. **Importance of High-Quality Data**
   
### 2. **Pre-processing Steps**
   
### 3. **Tools for Dataset Analysis**
   
### 4. **Downstream Impact**


## Vocabulary & Tokenizer

### 1. **Vocabulary**
- Describes the process of determining the set of words or symbols the model will recognize.
### 2. **Tokenization**
- Explains the rules for breaking down text into manageable pieces, or tokens, for the model to process.
### 3. **Mismatch Between Human and Machine Processing**
- Discusses the implications of discrepancies between how humans and models process language, particularly in terms of meaning.
### 4. **Downstream Impact**
- Examines how mismatches in vocabulary and tokenization can affect the model's performance on real-world tasks.

## Learning Objective

  1. **General Skills Targeted**: Details the types of skills, such as syntax, semantics, and reasoning, that pre-training aims to instill in language models.
  2. **Variety of Learning Objectives**: Outlines the different tasks used in pre-training to develop these skills.
  3. **Task Performance Impact**: Investigates whether models are better at tasks similar to those they were pre-trained on.
  4. **Assumptions about Task Suitability**: Challenges the assumption that pre-training tasks directly correlate with improved performance on related downstream tasks.

## Architecture

### 1. **Transformer Architecture**
- Introduces the Transformer as the foundation for most modern language models.
### 2. **Variants of Transformer Models**
- Describes the characteristics and uses of encoder-only, decoder-only, and encoder-decoder models.
### 3. **Rationale for Architectural Choices**
- Discusses why certain architectures are preferred for specific types of language model tasks.
### 4. **Impact on Model Performance**
- Considers how the choice of architecture influences the capabilities and performance of language models.



Lately, model providers have been augmenting the base model by tuning it on much smaller datasets in order to steer them towards being more aligned with human needs and preferences. Some popular tuning modes are:

- upervised instruction fine-tuning, so that the model is better at following human instructions.

- RLHF (Reinforcement Learning by Human Feedback), so that the model is better aligned with human preferences.  (annotator)

- Domain-adaptive/ task-adaptive continued pre-training, so that the model is better attuned to specific domains and tasks.
  
- Based on the specific augmentation carried out, the resulting models are called instruct models, chat models and so on.



What Are Text Generation Models?
Historical Underpinnings: The Rise of Transformer Architectures
OpenAI’s Generative Pre-trained Transformers
GPT-3.5-turbo and GPT-4
GPT-4
Google’s Bard
Meta’s LLaMA and LLMA2
Mistral 7B
Anthropic - Claude
Model Comparison

