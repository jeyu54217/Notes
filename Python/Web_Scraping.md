160.列举您使用过的Python网络爬虫所用到的网络数据包?
requests, urllib,urllib2, httplib2

161.爬取数据后使用哪个数据库存储数据的，为什么？
162.你用过的爬虫框架或者模块有哪些？优缺点？
Python自带：urllib,urllib2

第三方：requests

框架： Scrapy

urllib 和urllib2模块都做与请求URL相关的操作，但他们提供不同的功能。

urllib2: urllib2.urlopen可以接受一个Request对象或者url,(在接受Request对象时，并以此可以来设置一个URL的headers),urllib.urlopen只接收一个url。

urllib 有urlencode,urllib2没有，因此总是urllib, urllib2常会一起使用的原因

scrapy是封装起来的框架，他包含了下载器，解析器，日志及异常处理，基于多线程，twisted的方式处理，对于固定单个网站的爬取开发，有优势，但是对于多网站爬取100个网站，并发及分布式处理不够灵活，不便调整与扩展

requests是一个HTTP库，它只是用来请求，它是一个强大的库，下载，解析全部自己处理，灵活性高

Scrapy优点：异步，xpath，强大的统计和log系统，支持不同url。shell方便独立调试。写middleware方便过滤。通过管道存入数据库

163.写爬虫是用多进程好？还是多线程好？
164.常见的反爬虫和应对方法？
165.解析网页的解析器使用最多的是哪几个?
166.需要登录的网页，如何解决同时限制ip，cookie,session
167.验证码的解决?
168.使用最多的数据库，对他们的理解？
169.编写过哪些爬虫中间件？
170.“极验”滑动验证码如何破解？
171.爬虫多久爬一次，爬下来的数据是怎么存储？
172.cookie过期的处理问题？
173.动态加载又对及时性要求很高怎么处理？
174.HTTPS有什么优点和缺点？
175.HTTPS是如何实现安全传输数据的？
176.TTL，MSL，RTT各是什么？
177.谈一谈你对Selenium和PhantomJS了解
178.平常怎么使用代理的 ？
179.存放在数据库(redis、mysql等)。
180.怎么监控爬虫的状态?
181.描述下scrapy框架运行的机制？
182.谈谈你对Scrapy的理解？
183.怎么样让 scrapy 框架发送一个 post 请求（具体写出来）
184.怎么监控爬虫的状态 ？
185.怎么判断网站是否更新？
186.图片、视频爬取怎么绕过防盗连接
187.你爬出来的数据量大概有多大？大概多长时间爬一次？
188.用什么数据库存爬下来的数据？部署是你做的吗？怎么部署？
189.增量爬取
190.爬取下来的数据如何去重，说一下scrapy的具体的算法依据。
191.Scrapy的优缺点?
192.怎么设置爬取深度？
193.scrapy和scrapy-redis有什么区别？为什么选择redis数据库？
194.分布式爬虫主要解决什么问题？
195.什么是分布式存储？
196.你所知道的分布式爬虫方案有哪些？
197.scrapy-redis，有做过其他的分布式爬虫吗？